{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "from collections import namedtuple\n",
    "from itertools import chain, repeat\n",
    "from pprint import pformat, pprint\n",
    "from typing import (\n",
    "    AbstractSet, Any, Callable, Collection, Dict, Generator, Generic, Iterable, List, Mapping,\n",
    "    NamedTuple, Optional, Tuple, TypeVar, Union\n",
    ")\n",
    "\n",
    "from graphql import GraphQLList, GraphQLString, parse\n",
    "from graphql.utilities.build_ast_schema import build_ast_schema\n",
    "from graphql_compiler.compiler.compiler_frontend import graphql_to_ir\n",
    "from graphql_compiler.compiler.blocks import (\n",
    "    Backtrack, CoerceType, ConstructResult, EndOptional, Filter, GlobalOperationsStart, \n",
    "    MarkLocation, Recurse, QueryRoot, Traverse\n",
    ")\n",
    "from graphql_compiler.compiler.compiler_entities import BasicBlock, Expression\n",
    "from graphql_compiler.compiler.expressions import (\n",
    "    BinaryComposition, ContextField, ContextFieldExistence, Literal, LocalField, \n",
    "    OutputContextField, TernaryConditional, Variable\n",
    ")\n",
    "from graphql_compiler.compiler.compiler_frontend import IrAndMetadata, graphql_to_ir\n",
    "from graphql_compiler.compiler.helpers import Location, get_only_element_from_collection\n",
    "from graphql_compiler.schema import GraphQLDate, GraphQLDateTime, GraphQLDecimal\n",
    "from graphql_compiler.tests.test_helpers import SCHEMA_TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_empty_stack():\n",
    "    return ImmutableStack(None, 0, None)\n",
    "    \n",
    "\n",
    "class ImmutableStack(NamedTuple):\n",
    "    value: Any\n",
    "    depth: int\n",
    "    tail: Optional['ImmutableStack']\n",
    "        \n",
    "    def peek(self) -> Any:\n",
    "        return self.value\n",
    "    \n",
    "    def push(self, value: Any):\n",
    "        return ImmutableStack(value, self.depth + 1, self)\n",
    "    \n",
    "    def pop(self) -> Tuple[Any, 'ImmutableStack']:\n",
    "        return (self.value, self.tail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FilterInfo = namedtuple(\n",
    "    'FilterInfo',\n",
    "    ('field_name', 'op_name', 'value'),\n",
    ")\n",
    "DataToken = TypeVar('DataToken')\n",
    "\n",
    "\n",
    "class DataContext(Generic[DataToken]):\n",
    "    \n",
    "    __slots__ = (\n",
    "        'current_token',\n",
    "        'token_at_location',\n",
    "        'expression_stack',\n",
    "    )\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        current_token: Optional[DataToken], \n",
    "        token_at_location: Dict[Location, Optional[DataToken]], \n",
    "        expression_stack: ImmutableStack,\n",
    "    ):\n",
    "        self.current_token = current_token\n",
    "        self.token_at_location = token_at_location\n",
    "        self.expression_stack = expression_stack\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'DataContext(current={}, locations={}, stack={})'.format(\n",
    "            self.current_token, pformat(self.token_at_location), pformat(self.expression_stack))\n",
    "        \n",
    "    __str__ = __repr__\n",
    "        \n",
    "    @staticmethod\n",
    "    def make_empty_context_from_token(token: DataToken) -> 'DataContext':\n",
    "        return DataContext(token, dict(), make_empty_stack())\n",
    "    \n",
    "    def push_value_onto_stack(self, value: Any) -> 'DataContext':\n",
    "        self.expression_stack = self.expression_stack.push(value)\n",
    "        return self  # for chaining\n",
    "    \n",
    "    def peek_value_on_stack(self) -> Any:\n",
    "        return self.expression_stack.peek()\n",
    "        \n",
    "    def pop_value_from_stack(self) -> Any:\n",
    "        value, remaining_stack = self.expression_stack.pop()\n",
    "        self.expression_stack = remaining_stack\n",
    "        return value\n",
    "    \n",
    "    def get_context_for_location(self, location: Location) -> 'DataContext':\n",
    "        return DataContext(\n",
    "            self.token_at_location[location], \n",
    "            dict(self.token_at_location), \n",
    "            self.expression_stack,\n",
    "        )\n",
    "        \n",
    "\n",
    "class InterpreterAdapter(Generic[DataToken], metaclass=ABCMeta):\n",
    "    @abstractmethod\n",
    "    def get_tokens_of_type(\n",
    "        self,\n",
    "        type_name: str, \n",
    "        **hints\n",
    "    ) -> Iterable[DataToken]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def project_property(\n",
    "        self,\n",
    "        data_contexts: Iterable[DataContext], \n",
    "        field_name: str,\n",
    "        **hints\n",
    "    ) -> Iterable[Tuple[DataContext, Any]]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def project_neighbors(\n",
    "        self,\n",
    "        data_contexts: Iterable[DataContext], \n",
    "        direction: str,\n",
    "        edge_name: str, \n",
    "        **hints\n",
    "    ) -> Iterable[Tuple[DataContext, Iterable[DataToken]]]:\n",
    "        # If using a generator instead of a list for the Iterable[DataToken] part,\n",
    "        # be careful -- generators are not closures! Make sure any state you pull into\n",
    "        # the generator from the outside does not change, or that bug will be hard to find.\n",
    "        # Remember: it's always safer to use a function to produce the generator, since\n",
    "        # that will explicitly preserve all the external values passed into it.\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def can_coerce_to_type(\n",
    "        self,\n",
    "        data_contexts: Iterable[DataContext], \n",
    "        type_name: str,\n",
    "        **hints\n",
    "    ) -> Iterable[Tuple[DataContext, bool]]:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_operator(operator: str, left_value: Any, right_value: Any) -> Any:\n",
    "    if operator == '=':\n",
    "        return left_value == right_value\n",
    "    elif operator == 'contains':\n",
    "        return right_value in left_value\n",
    "    else:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _push_values_onto_data_context_stack(\n",
    "    contexts_and_values: Iterable[Tuple[DataContext, Any]]\n",
    ") -> Iterable[DataContext]:\n",
    "    return (\n",
    "        data_context.push_value_onto_stack(value)\n",
    "        for data_context, value in contexts_and_values\n",
    "    )\n",
    "\n",
    "\n",
    "def _evaluate_binary_composition(\n",
    "    adapter: InterpreterAdapter[DataToken],\n",
    "    query_arguments: Dict[str, Any],\n",
    "    expression: BinaryComposition,\n",
    "    data_contexts: Iterable[DataContext],\n",
    ") -> Iterable[Tuple[DataContext, Any]]:\n",
    "    data_contexts = _push_values_onto_data_context_stack(\n",
    "        _evaluate_expression(adapter, query_arguments, expression.left, data_contexts)\n",
    "    )\n",
    "    data_contexts = _push_values_onto_data_context_stack(\n",
    "        _evaluate_expression(adapter, query_arguments, expression.right, data_contexts)\n",
    "    )\n",
    "    \n",
    "    for data_context in data_contexts:\n",
    "        # N.B.: The left sub-expression is evaluated first, therefore its value in the stack\n",
    "        #       is *below* the value of the right sub-expression.\n",
    "        #       These two lines cannot be inlined into the _apply_operator() call since\n",
    "        #       the popping order there would be incorrectly reversed.\n",
    "        right_value = data_context.pop_value_from_stack()\n",
    "        left_value = data_context.pop_value_from_stack()\n",
    "        final_expression_value = _apply_operator(expression.operator, left_value, right_value)\n",
    "        yield (data_context, final_expression_value)\n",
    "\n",
    "\n",
    "def _evaluate_ternary_conditional(\n",
    "    adapter: InterpreterAdapter[DataToken],\n",
    "    query_arguments: Dict[str, Any],\n",
    "    expression: TernaryConditional,\n",
    "    data_contexts: Iterable[DataContext],\n",
    ") -> Iterable[Tuple[DataContext, Any]]:\n",
    "    # TODO(predrag): Try to optimize this to avoid evaluating sides of expressions we might not use.\n",
    "    data_contexts = _push_values_onto_data_context_stack(\n",
    "        _evaluate_expression(adapter, query_arguments, expression.predicate, data_contexts)\n",
    "    )\n",
    "    data_contexts = _push_values_onto_data_context_stack(\n",
    "        _evaluate_expression(adapter, query_arguments, expression.if_true, data_contexts)\n",
    "    )\n",
    "    data_contexts = _push_values_onto_data_context_stack(\n",
    "        _evaluate_expression(adapter, query_arguments, expression.if_false, data_contexts)\n",
    "    )\n",
    "    \n",
    "    for data_context in data_contexts:\n",
    "        # N.B.: The expression evaluation order is \"predicate, if_true, if_false\", and since the\n",
    "        #       results are pushed onto a stack (LIFO order), the pop order has to be inverted.\n",
    "        if_false_value = data_context.pop_value_from_stack()\n",
    "        if_true_value = data_context.pop_value_from_stack()\n",
    "        predicate_value = data_context.pop_value_from_stack()\n",
    "        result_value = if_true_value if predicate_value else if_false_value\n",
    "        yield (data_context, result_value)\n",
    "\n",
    "        \n",
    "def _evaluate_local_field(\n",
    "    adapter: InterpreterAdapter[DataToken], \n",
    "    query_arguments: Dict[str, Any],\n",
    "    expression: LocalField,\n",
    "    data_contexts: Iterable[DataContext],\n",
    ") -> Iterable[Tuple[DataContext, Any]]:\n",
    "    field_name = expression.field_name\n",
    "    return adapter.project_property(data_contexts, field_name)\n",
    "\n",
    "\n",
    "def _evaluate_context_field(\n",
    "    adapter: InterpreterAdapter[DataToken], \n",
    "    query_arguments: Dict[str, Any],\n",
    "    expression: Union[ContextField, OutputContextField],\n",
    "    data_contexts: Iterable[DataContext],\n",
    ") -> Iterable[Tuple[DataContext, Any]]:\n",
    "    location = expression.location.at_vertex()\n",
    "    field_name = expression.location.field\n",
    "    \n",
    "    moved_contexts = (\n",
    "        data_context.get_context_for_location(location).push_value_onto_stack(data_context)\n",
    "        for data_context in data_contexts\n",
    "    )\n",
    "    \n",
    "    return (\n",
    "        (moved_data_context.pop_value_from_stack(), value)\n",
    "        for moved_data_context, value in adapter.project_property(moved_contexts, field_name)\n",
    "    )\n",
    "\n",
    "\n",
    "def _evaluate_context_field_existence(\n",
    "    adapter: InterpreterAdapter[DataToken], \n",
    "    query_arguments: Dict[str, Any],\n",
    "    expression: ContextFieldExistence,\n",
    "    data_contexts: Iterable[DataContext],\n",
    ") -> Iterable[Tuple[DataContext, Any]]:\n",
    "    location = expression.location.at_vertex()\n",
    "    \n",
    "    for data_context in data_contexts:\n",
    "        existence_value = data_context.token_at_location[location] is not None\n",
    "        yield (data_context, existence_value)\n",
    "\n",
    "    \n",
    "def _evaluate_variable(\n",
    "    adapter: InterpreterAdapter[DataToken], \n",
    "    query_arguments: Dict[str, Any],\n",
    "    expression: Variable,\n",
    "    data_contexts: Iterable[DataContext],\n",
    ") -> Iterable[Tuple[DataContext, Any]]:\n",
    "    variable_value = query_arguments[expression.variable_name[1:]]\n",
    "    return (\n",
    "        (data_context, variable_value)\n",
    "        for data_context in data_contexts\n",
    "    )\n",
    "\n",
    "\n",
    "def _evaluate_literal(\n",
    "    adapter: InterpreterAdapter[DataToken], \n",
    "    query_arguments: Dict[str, Any],\n",
    "    expression: Literal,\n",
    "    data_contexts: Iterable[DataContext],\n",
    ") -> Iterable[Tuple[DataContext, Any]]:\n",
    "    return (\n",
    "        (data_context, expression.value)\n",
    "        for data_context in data_contexts\n",
    "    )\n",
    "\n",
    "\n",
    "def _evaluate_expression(\n",
    "    adapter: InterpreterAdapter[DataToken],\n",
    "    query_arguments: Dict[str, Any],\n",
    "    expression: Expression,\n",
    "    data_contexts: Iterable[DataContext],\n",
    ") -> Iterable[Tuple[DataContext, Any]]:\n",
    "    type_to_handler = {\n",
    "        BinaryComposition: _evaluate_binary_composition,\n",
    "        TernaryConditional: _evaluate_ternary_conditional,\n",
    "        ContextField: _evaluate_context_field,\n",
    "        OutputContextField: _evaluate_context_field,\n",
    "        LocalField: _evaluate_local_field,\n",
    "        ContextFieldExistence: _evaluate_context_field_existence,\n",
    "        Variable: _evaluate_variable,\n",
    "        Literal: _evaluate_literal,\n",
    "    }\n",
    "    expression_type = type(expression)\n",
    "    return type_to_handler[expression_type](adapter, query_arguments, expression, data_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _handle_filter(\n",
    "    adapter: InterpreterAdapter[DataToken], \n",
    "    query_arguments: Dict[str, Any],\n",
    "    block: Filter,\n",
    "    data_contexts: Iterable[DataContext],\n",
    ") -> Iterable[DataContext]:\n",
    "    predicate = block.predicate\n",
    "    \n",
    "    # TODO(predrag): Handle the \"filters depending on missing optional values pass\" rule.\n",
    "    \n",
    "    yield from (\n",
    "        data_context\n",
    "        for data_context, predicate_value in _evaluate_expression(\n",
    "            adapter, query_arguments, predicate, data_contexts\n",
    "        )\n",
    "        if predicate_value or data_context.current_token is None\n",
    "    )\n",
    "    \n",
    "\n",
    "def _handle_traverse(\n",
    "    adapter: InterpreterAdapter[DataToken],\n",
    "    query_arguments: Dict[str, Any],\n",
    "    block: Traverse,\n",
    "    data_contexts: Iterable[DataContext],\n",
    ") -> Iterable[DataContext]:\n",
    "    neighbor_data = adapter.project_neighbors(data_contexts, block.direction, block.edge_name)\n",
    "    for data_context, neighbor_tokens in neighbor_data:\n",
    "        has_neighbors = False\n",
    "        for neighbor_token in neighbor_tokens:\n",
    "            has_neighbors = True\n",
    "            yield (\n",
    "                # TODO(predrag): Make a helper staticmethod on DataContext for this.\n",
    "                DataContext(\n",
    "                    neighbor_token, data_context.token_at_location, data_context.expression_stack\n",
    "                )\n",
    "            )\n",
    "        if block.optional and not has_neighbors:\n",
    "            yield DataContext(\n",
    "                None, data_context.token_at_location, data_context.expression_stack\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _handle_recurse(\n",
    "    adapter: InterpreterAdapter[DataToken],\n",
    "    query_arguments: Dict[str, Any],\n",
    "    block: Recurse,\n",
    "    data_contexts: Iterable[DataContext],\n",
    ") -> Iterable[DataContext]:\n",
    "    data_contexts = _handle_already_inactive_tokens(data_contexts)\n",
    "    \n",
    "    for current_depth in range(block.depth):\n",
    "        data_contexts = _iterative_recurse_handler(\n",
    "            adapter, query_arguments, block, data_contexts, current_depth\n",
    "        )\n",
    "        \n",
    "    return _unwrap_recursed_data_contexts(data_contexts)\n",
    "    \n",
    "    \n",
    "def _handle_already_inactive_tokens(\n",
    "    data_contexts: Iterable[DataContext],\n",
    ") -> Iterable[DataContext]:\n",
    "    for data_context in data_contexts:\n",
    "        current_token = data_context.current_token\n",
    "        if current_token is None:\n",
    "            # Got a context that is already deactivated at the start of the recursion.\n",
    "            # Push \"None\" onto the stack to make sure it remains deactivated at the end of it too.\n",
    "            data_context.push_value_onto_stack(None)\n",
    "        yield data_context\n",
    "    \n",
    "    \n",
    "def _iterative_recurse_handler(\n",
    "    adapter: InterpreterAdapter[DataToken],\n",
    "    query_arguments: Dict[str, Any],\n",
    "    block: Recurse,\n",
    "    data_contexts: Iterable[DataContext],\n",
    "    current_depth: int,\n",
    ") -> Iterable[DataContext]:\n",
    "    neighbor_data = adapter.project_neighbors(data_contexts, block.direction, block.edge_name)\n",
    "    for data_context, neighbor_tokens in neighbor_data:\n",
    "        yield from (\n",
    "            # TODO(predrag): Make a helper staticmethod on DataContext for this.\n",
    "            DataContext(\n",
    "                neighbor_token, data_context.token_at_location, data_context.expression_stack\n",
    "            )\n",
    "            for neighbor_token in neighbor_tokens\n",
    "        )\n",
    "        current_token = data_context.current_token\n",
    "        if current_token is None:\n",
    "            # The context is already inactive so its neighbors\n",
    "            # will not be visited in the next iteration.\n",
    "            yield data_context\n",
    "        else:\n",
    "            # We just visited this context's neighbors, deactivate the context\n",
    "            # so we don't end up visiting them again in the next iteration.\n",
    "            data_context.push_value_onto_stack(current_token)\n",
    "            yield DataContext(\n",
    "                None, data_context.token_at_location, data_context.expression_stack\n",
    "            )\n",
    "        \n",
    "\n",
    "def _unwrap_recursed_data_contexts(\n",
    "    data_contexts: Iterable[DataContext]\n",
    ") -> Iterable[DataContext]:\n",
    "    for data_context in data_contexts:\n",
    "        if data_context.current_token is not None:\n",
    "            # Got a still-active context, produce it as-is.\n",
    "            yield data_context\n",
    "        else:\n",
    "            # Got an inactivated context, reactivate it by replacing the token from the stack.\n",
    "            current_token = data_context.pop_value_from_stack()\n",
    "            yield DataContext(\n",
    "                current_token, data_context.token_at_location, data_context.expression_stack\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _produce_output(\n",
    "    adapter: InterpreterAdapter[DataToken], \n",
    "    query_arguments: Dict[str, Any],\n",
    "    output_name: str,\n",
    "    output_expression: Expression,\n",
    "    data_contexts: Iterable[DataContext],\n",
    ") -> Iterable[DataContext]:\n",
    "    data_contexts = _print_tap(\n",
    "        'outputting ' + output_name, data_contexts)\n",
    "    \n",
    "    contexts_and_values = _evaluate_expression(\n",
    "        adapter, query_arguments, output_expression, data_contexts)\n",
    "    \n",
    "    for data_context, value in contexts_and_values:\n",
    "        data_context.peek_value_on_stack()[output_name] = value\n",
    "        yield data_context\n",
    "    \n",
    "\n",
    "def _handle_construct_result(\n",
    "    adapter: InterpreterAdapter[DataToken], \n",
    "    query_arguments: Dict[str, Any],\n",
    "    block: ConstructResult,\n",
    "    data_contexts: Iterable[DataContext],\n",
    ") -> Iterable[Dict[str, Any]]:\n",
    "    output_fields = block.fields\n",
    "    \n",
    "    data_contexts = (\n",
    "        data_context.push_value_onto_stack(dict())\n",
    "        for data_context in data_contexts\n",
    "    )\n",
    "    \n",
    "    for output_name, output_expression in output_fields.items():\n",
    "        data_contexts = _produce_output(\n",
    "            adapter, query_arguments, output_name, output_expression, data_contexts)\n",
    "        \n",
    "    return (\n",
    "        data_context.pop_value_from_stack()\n",
    "        for data_context in data_contexts\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _handle_coerce_type(\n",
    "    adapter: InterpreterAdapter[DataToken],\n",
    "    query_arguments: Dict[str, Any],\n",
    "    block: CoerceType,\n",
    "    data_contexts: Iterable[DataContext],\n",
    ") -> Iterable[DataContext]:\n",
    "    coercion_type = get_only_element_from_collection(block.target_class)\n",
    "    return (\n",
    "        data_context\n",
    "        for data_context, can_coerce in adapter.can_coerce_to_type(data_contexts, coercion_type)\n",
    "        if can_coerce or data_context.current_token is None\n",
    "    )\n",
    "    \n",
    "\n",
    "def _handle_mark_location(\n",
    "    adapter: InterpreterAdapter[DataToken],\n",
    "    query_arguments: Dict[str, Any],\n",
    "    block: MarkLocation,\n",
    "    data_contexts: Iterable[DataContext],\n",
    ") -> Iterable[DataContext]:\n",
    "    current_location = block.location\n",
    "    for data_context in data_contexts:\n",
    "        token_at_location = dict(data_context.token_at_location)\n",
    "        token_at_location[current_location] = data_context.current_token\n",
    "        yield DataContext(\n",
    "            data_context.current_token,\n",
    "            token_at_location,\n",
    "            data_context.expression_stack,\n",
    "        )\n",
    "        \n",
    "\n",
    "def _handle_backtrack(\n",
    "    adapter: InterpreterAdapter[DataToken],\n",
    "    query_arguments: Dict[str, Any],\n",
    "    block: Backtrack,\n",
    "    data_contexts: Iterable[DataContext],\n",
    ") -> Iterable[DataContext]:\n",
    "    backtrack_location = block.location\n",
    "    for data_context in data_contexts:\n",
    "        yield DataContext(\n",
    "            data_context.token_at_location[backtrack_location],\n",
    "            data_context.token_at_location,\n",
    "            data_context.expression_stack,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _handle_block(\n",
    "    adapter: InterpreterAdapter[DataToken],\n",
    "    query_arguments: Dict[str, Any],\n",
    "    block: BasicBlock,\n",
    "    data_contexts: Iterable[DataContext],\n",
    ") -> Iterable[DataContext]:\n",
    "    no_op_types = (EndOptional, GlobalOperationsStart,)\n",
    "    if isinstance(block, no_op_types):\n",
    "        return data_contexts\n",
    "    \n",
    "    data_contexts = _print_tap('pre: ' + str(block), data_contexts)\n",
    "    \n",
    "    handler_functions = {        \n",
    "        CoerceType: _handle_coerce_type,\n",
    "        Filter: _handle_filter,\n",
    "        MarkLocation: _handle_mark_location,\n",
    "        Traverse: _handle_traverse,\n",
    "        Recurse: _handle_recurse,\n",
    "        Backtrack: _handle_backtrack,\n",
    "    }\n",
    "    return handler_functions[type(block)](adapter, query_arguments, block, data_contexts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _print_tap(info: str, data_contexts: Iterable[DataContext]) -> Iterable[DataContext]:\n",
    "    return data_contexts\n",
    "#     print('\\n')\n",
    "#     unique_id = hash(info)\n",
    "#     print(unique_id, info)\n",
    "#     from funcy.py3 import chunks\n",
    "#     for context_chunk in chunks(100, data_contexts):\n",
    "#         for context in context_chunk:\n",
    "#             pprint((unique_id, context))\n",
    "#             yield context\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_ir(\n",
    "    adapter: InterpreterAdapter[DataToken], \n",
    "    ir_and_metadata: IrAndMetadata, \n",
    "    query_arguments: Dict[str, Any]\n",
    ") -> Iterable[Dict[str, Any]]:\n",
    "    ir_blocks = ir_and_metadata.ir_blocks\n",
    "    query_metadata_table = ir_and_metadata.query_metadata_table\n",
    "    \n",
    "    if not ir_blocks:\n",
    "        raise AssertionError()\n",
    "        \n",
    "    first_block = ir_blocks[0]\n",
    "    if not isinstance(first_block, QueryRoot):\n",
    "        raise AssertionError()\n",
    "        \n",
    "    last_block = ir_blocks[-1]\n",
    "    if not isinstance(last_block, ConstructResult):\n",
    "        raise AssertionError()\n",
    "        \n",
    "    middle_blocks = ir_blocks[1:-1]\n",
    "        \n",
    "    start_class = get_only_element_from_collection(first_block.start_class)\n",
    "    \n",
    "    current_data_contexts = (\n",
    "        DataContext.make_empty_context_from_token(token)\n",
    "        for token in adapter.get_tokens_of_type(start_class)\n",
    "    )\n",
    "    \n",
    "    current_data_contexts = _print_tap('starting contexts', current_data_contexts)\n",
    "    \n",
    "    for block in middle_blocks:\n",
    "        current_data_contexts = _handle_block(\n",
    "            adapter, query_arguments, block, current_data_contexts)\n",
    "        \n",
    "    current_data_contexts = _print_tap('ending contexts', current_data_contexts)\n",
    "        \n",
    "    return _handle_construct_result(\n",
    "        adapter, query_arguments, last_block, current_data_contexts)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices = {\n",
    "    'Animal': [\n",
    "        {'name': 'Scooby Doo', 'uuid': '1001'},\n",
    "        {'name': 'Hedwig', 'uuid': '1002'},\n",
    "        {'name': 'Beethoven', 'uuid': '1003'},\n",
    "        {'name': 'Pongo', 'uuid': '1004'},\n",
    "        {'name': 'Perdy', 'uuid': '1005'},\n",
    "        {'name': 'Dipstick', 'uuid': '1006'},\n",
    "        {'name': 'Dottie', 'uuid': '1007'},\n",
    "        {'name': 'Domino', 'uuid': '1008'},\n",
    "        {'name': 'Little Dipper', 'uuid': '1009'},\n",
    "        {'name': 'Oddball', 'uuid': '1010'},\n",
    "    ],\n",
    "}\n",
    "edges = {\n",
    "    'Animal_ParentOf': [\n",
    "        ('1004', '1006'),\n",
    "        ('1005', '1006'),\n",
    "        ('1006', '1008'),\n",
    "        ('1006', '1009'),\n",
    "        ('1006', '1010'),\n",
    "        ('1007', '1008'),\n",
    "        ('1007', '1009'),\n",
    "        ('1007', '1010'),\n",
    "    ],\n",
    "}\n",
    "\n",
    "vertices_by_uuid = {\n",
    "    vertex['uuid']: vertex\n",
    "    for vertex in chain.from_iterable(vertices.values())\n",
    "}\n",
    "\n",
    "\n",
    "class InMemoryAdapter(InterpreterAdapter[dict]):\n",
    "    def get_tokens_of_type(\n",
    "        self,\n",
    "        type_name: str, \n",
    "        **hints\n",
    "    ) -> Iterable[dict]:\n",
    "        return vertices[type_name]\n",
    "\n",
    "    def project_property(\n",
    "        self,\n",
    "        data_contexts: Iterable[DataContext], \n",
    "        field_name: str,\n",
    "        **hints\n",
    "    ) -> Iterable[Tuple[DataContext, Any]]:\n",
    "        for data_context in data_contexts:\n",
    "            current_token = data_context.current_token\n",
    "            current_value = current_token[field_name] if current_token is not None else None\n",
    "            yield (data_context, current_value)\n",
    "\n",
    "    def project_neighbors(\n",
    "        self,\n",
    "        data_contexts: Iterable[DataContext], \n",
    "        direction: str,\n",
    "        edge_name: str, \n",
    "        **hints\n",
    "    ) -> Iterable[Tuple[DataContext, Iterable[DataToken]]]:\n",
    "        edge_info = edges[edge_name]\n",
    "        \n",
    "        for data_context in data_contexts:\n",
    "            neighbor_tokens = []\n",
    "            current_token = data_context.current_token\n",
    "            if current_token is not None:\n",
    "                uuid = current_token['uuid']\n",
    "                if direction == 'out':\n",
    "                    neighbor_tokens = [\n",
    "                        vertices_by_uuid[destination_uuid]\n",
    "                        for source_uuid, destination_uuid in edge_info\n",
    "                        if source_uuid == uuid\n",
    "                    ]\n",
    "                elif direction == 'in':\n",
    "                    neighbor_tokens = [\n",
    "                        vertices_by_uuid[source_uuid]\n",
    "                        for source_uuid, destination_uuid in edge_info\n",
    "                        if destination_uuid == uuid\n",
    "                    ]\n",
    "                else:\n",
    "                    raise AssertionError()\n",
    "                    \n",
    "            yield (data_context, neighbor_tokens)\n",
    "\n",
    "    def can_coerce_to_type(\n",
    "        self,\n",
    "        data_contexts: Iterable[DataContext], \n",
    "        type_name: str,\n",
    "        **hints\n",
    "    ) -> Iterable[Tuple[DataContext, bool]]:\n",
    "        # TODO(predrag): See if a redesign can make this be a no-op again.\n",
    "        return zip(data_contexts, repeat(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = build_ast_schema(parse(SCHEMA_TEXT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "{\n",
    "    Animal {\n",
    "        name @output(out_name: \"animal_name\")\n",
    "        uuid @output(out_name: \"animal_uuid\")\n",
    "    }\n",
    "}\n",
    "'''\n",
    "query_arguments = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "{\n",
    "    Animal {\n",
    "        name @output(out_name: \"parent_name\")\n",
    "\n",
    "        out_Animal_ParentOf {\n",
    "            name @output(out_name: \"child_name\")\n",
    "        }\n",
    "    }\n",
    "}\n",
    "'''\n",
    "query_arguments = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''{\n",
    "    Animal {\n",
    "        name @output(out_name: \"parent_name\")\n",
    "        \n",
    "        out_Animal_ParentOf @optional {\n",
    "            name @filter(op_name: \"in_collection\", value: [\"$child_names\"])\n",
    "                 @output(out_name: \"child_name\")\n",
    "        }\n",
    "    }\n",
    "}'''\n",
    "query_arguments = {\n",
    "    \"child_names\": ['Domino', 'Dipstick', 'Oddball'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''{\n",
    "    Animal {\n",
    "        name @output(out_name: \"ancestor_or_self_name\")\n",
    "        \n",
    "        out_Animal_ParentOf @recurse(depth: 4) {\n",
    "            name @filter(op_name: \"in_collection\", value: [\"$names\"])\n",
    "                 @output(out_name: \"descendant_or_self_name\")\n",
    "        }\n",
    "    }\n",
    "}'''\n",
    "query_arguments = {\n",
    "    \"names\": ['Domino', 'Dipstick', 'Oddball'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''{\n",
    "    Animal {\n",
    "        name @output(out_name: \"ancestor_or_self_name\")\n",
    "        \n",
    "        out_Animal_ParentOf @recurse(depth: 1) {\n",
    "            name @filter(op_name: \"in_collection\", value: [\"$names\"])\n",
    "                 @output(out_name: \"descendant_or_self_name\")\n",
    "        }\n",
    "    }\n",
    "}'''\n",
    "query_arguments = {\n",
    "    \"names\": ['Domino', 'Dipstick', 'Oddball'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "{\n",
    "    Animal {\n",
    "        name @output(out_name: \"parent_name\")\n",
    "\n",
    "        out_Animal_ParentOf {\n",
    "            name @filter(op_name: \"in_collection\", value: [\"$child_names\"])\n",
    "                 @output(out_name: \"child_name\")\n",
    "        }\n",
    "    }\n",
    "}\n",
    "'''\n",
    "query_arguments = {\n",
    "    \"child_names\": ['Domino', 'Dipstick', 'Oddball'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "{\n",
    "    Animal {\n",
    "        name @output(out_name: \"grandparent_name\")\n",
    "\n",
    "        out_Animal_ParentOf {\n",
    "            name @output(out_name: \"parent_name\")\n",
    "            \n",
    "            out_Animal_ParentOf {\n",
    "                name @output(out_name: \"child_name\")\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "'''\n",
    "query_arguments = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''{\n",
    "    Animal {\n",
    "        name @output(out_name: \"start\") @filter(op_name: \"=\", value: [\"$start\"])\n",
    "        \n",
    "        in_Animal_ParentOf @recurse(depth: 1) {\n",
    "            name @output(out_name: \"first_recursion\")\n",
    "        }\n",
    "    }\n",
    "}'''\n",
    "query_arguments = {\n",
    "    \"start\": \"Dipstick\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QueryRoot(({'Animal'},)),\n",
      " Filter((BinaryComposition(('=', LocalField(('name', <GraphQLScalarType 'String'>)), Variable(('$start', <GraphQLScalarType 'String'>)))),)),\n",
      " MarkLocation((Location(('Animal',), None, 1),)),\n",
      " Recurse(('in', 'Animal_ParentOf', 1), {'within_optional_scope': False}),\n",
      " MarkLocation((Location(('Animal', 'in_Animal_ParentOf'), None, 1),)),\n",
      " Backtrack((Location(('Animal',), None, 1),), {'optional': False}),\n",
      " GlobalOperationsStart(),\n",
      " ConstructResult(({'start': OutputContextField((Location(('Animal',), name, 1), <GraphQLScalarType 'String'>)), 'first_recursion': OutputContextField((Location(('Animal', 'in_Animal_ParentOf'), name, 1), <GraphQLScalarType 'String'>))},))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'start': 'Dipstick', 'first_recursion': 'Pongo'},\n",
       " {'start': 'Dipstick', 'first_recursion': 'Perdy'},\n",
       " {'start': 'Dipstick', 'first_recursion': 'Dipstick'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ir_and_metadata = graphql_to_ir(schema, query)\n",
    "pprint(ir_and_metadata.ir_blocks)\n",
    "\n",
    "result = list(interpret_ir(InMemoryAdapter(), ir_and_metadata, query_arguments))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
